---
title: "Final_attempt_6"
author: "Rahul Singh"
date: "2/18/2018"
output: html_document
---


Kaggle Score 0.12391

Username: rahulsinghrs7

Rank: 1107

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
library(purrr)
library(gtools)
library(visdat)
library(dplyr)
library(ggplot2)
library(plyr)
library(moments)
library(glmnet)
library(caret)
library(FSelector)
```

```{r}
train <- read.csv("~/Desktop/IE MBD Term2/Machine Learning 2/CODES/Practice 1/House Pricing Kaggle/train.csv",stringsAsFactors = FALSE)

test <- read.csv("~/Desktop/IE MBD Term2/Machine Learning 2/CODES/Practice 1/House Pricing Kaggle/test.csv",stringsAsFactors = FALSE)
```

```{r}
all_data <- rbind(select(train,MSSubClass:SaleCondition),
                  select(test,MSSubClass:SaleCondition))

```

```{r}
# get data frame of SalePrice and log(SalePrice + 1) for plotting
df <- rbind(data.frame(version="log(price+1)",x=log(train$SalePrice + 1)),
            data.frame(version="price",x=train$SalePrice))

# plot histogram tocheck for skewness
ggplot(data=df) +
  facet_wrap(~version,ncol=2,scales="free_x") +
  geom_histogram(aes(x=x))
```

```{r}
# transform SalePrice target to log form
train$SalePrice <- log(train$SalePrice + 1)

# store the type (character, interger, numeric, factor,etc) of each column in the combined dataset
feature_classes <- sapply(names(all_data),function(x){class(all_data[[x]])})
numeric_feats <-names(feature_classes[feature_classes != "character"])

# use skewness function that computes skewness of given data to know the skewness of each numerical type column in combined dataset
skewed_feats <- sapply(numeric_feats,function(x){skewness(all_data[[x]],na.rm=TRUE)})

# keep only those fauture which are numerical and have a skewness of more than 75%
skewed_feats <- skewed_feats[skewed_feats > 0.75]

# transform excessively skewed features with log(x + 1)
for(x in names(skewed_feats)) {
  all_data[[x]] <- log(all_data[[x]] + 1)
}
```

```{r}
# Then we get the names of columns which are categorical/ character type
categorical_feats <- names(feature_classes[feature_classes == "character"])

# convert all character types to numeric by using dummyVars function from caret library
dummies <- dummyVars(~.,all_data[categorical_feats])
categorical_1_hot <- predict(dummies,all_data[categorical_feats])

#replace all the NA values in converted categorical columns to 0
categorical_1_hot[is.na(categorical_1_hot)] <- 0  
```

```{r}
# for any missing values in numeric features, impute mean of that feature; from Kernel by J Thompson
numeric_df <- all_data[numeric_feats]

for (x in numeric_feats) {
    mean_value <- mean(train[[x]],na.rm = TRUE)
    all_data[[x]][is.na(all_data[[x]])] <- mean_value
}

```

```{r}
# so far all the categorical features are converted to dummy variables 
#the missing values in the columns which were numerical have been replaced by the mean
#we save all this info in new_combined_test_train dataframe where we column bind categorical and numerical features which have been processed
all_data <- cbind(all_data[numeric_feats],categorical_1_hot)
```

```{r}
#some new features
all_data$year_qual <- all_data$YearBuilt*all_data$OverallQual #overall condition
all_data$year_r_qual <- all_data$YearRemodAdd*all_data$OverallQual #quality x remodel
all_data$qual_bsmt <- all_data$OverallQual*all_data$TotalBsmtSF #quality x basement size
all_data$livarea_qual <- all_data$OverallQual*all_data$GrLivArea #quality x living area
```

```{r}
#we spilt the processed file into train_final and test_final
# create data for training and test
X_train <- all_data[1:nrow(train),]
X_test <- all_data[(nrow(train)+1):nrow(all_data),]
y <- train$SalePrice
```


```{r}
# set up caret model training parameters
# model specific training parameter
train_control_config <- trainControl(method="repeatedcv",
                                 number=5,
                                 repeats=5,
                                 verboseIter=FALSE)
```

```{r}
#Generate regular sequences of lambda from 1 to 0 with a reduction of -0.001 with each next value

lambdas <- seq(1,0,-0.001)

# train model
set.seed(123)  # for reproducibility
model_ridge <- train(x=X_train,y=y,
                  method="glmnet",
                  metric="RMSE",
                  maximize=FALSE,
                  trControl=train_control_config,
                  tuneGrid=expand.grid(alpha=0, # Ridge regression
                                       lambda=lambdas))
# α (alpha) is the parameter which balances the amount of emphasis given to minimizing RSS vs minimizing sum of square of coefficients
#With alpha equals zero: our objective becomes same as simple linear regression and we’ll get the same coefficients as simple linear regression.
```


```{r}
# test out Lasso regression model

# train model
set.seed(123)  # for reproducibility
model_lasso <- train(x=X_train,y=y,
                  method="glmnet",
                  metric="RMSE",
                  maximize=FALSE,
                  trControl=train_control_config,
                  tuneGrid=expand.grid(alpha=1,  # Lasso regression
                                       lambda=c(1,0.1,0.05,0.01,seq(0.009,0.001,-0.001),
                                            0.00075,0.0005,0.0001)))
model_lasso
```

```{r}
# we get the coefficients for the best performing model and store it in coef_of_best_performing_model 
coef <- data.frame(coef.name = dimnames(coef(model_lasso$finalModel,s=model_lasso$bestTune$lambda))[[1]], coef.value = matrix(coef(model_lasso$finalModel,s=model_lasso$bestTune$lambda)))

# We remove the intercept terms
coef <- coef[-1,]
```

```{r}
# print summary of model results
picked_features <- nrow(filter(coef,coef.value!=0))
not_picked_features <- nrow(filter(coef,coef.value==0))

cat("Lasso picked",picked_features,"variables and eliminated the other",
    not_picked_features,"variables\n")
```

```{r}
ggplot(data=filter(model_ridge$result,RMSE<0.14)) +
    geom_line(aes(x=lambda,y=RMSE))
```


```{r}
# sort coefficients in ascending order
coef <- arrange(coef,-coef.value)

# extract the top 10 and bottom 10 features
imp_coef <- rbind(head(coef,10),
                  tail(coef,10))
```

```{r}
#predict
preds <- exp(predict(model_lasso,newdata=X_test)) - 1
```

```{r}
solution <- data.frame(Id=as.integer(rownames(X_test)),SalePrice=preds)
write.csv(solution,"Final.csv",row.names=FALSE)
```

